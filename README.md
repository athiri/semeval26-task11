# SemEval 2026 Task 11 - Disentangling Content and Formal Reasoning
## Student Research Project - Complete Guide

> **Research Goal:** Build models that can assess formal validity of syllogistic arguments independent of their plausibility  
> **Competition:** [SemEval 2026 Task 11](https://sites.google.com/view/semeval-2026-task-11)  
> **Evaluation Period:** January 2026  
> **Paper Deadline:** February 2026  
> **Target:** High accuracy with low content effect bias

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“Š Training Data Overview

**Official Dataset:** Provided by SemEval 2026 Task 11 organizers

| Subtask | Description | Languages | Key Challenge |
|---------|-------------|-----------|---------------|
| **Subtask 1** | Syllogistic Reasoning | English | Validity prediction independent of plausibility |
| **Subtask 2** | + Irrelevant Premises | English | Identify relevant premises + validity |
| **Subtask 3** | Multilingual Reasoning | 12 languages | Cross-lingual validity prediction |
| **Subtask 4** | Multilingual + Irrelevant | 12 languages | Multilingual premise selection + validity |

**Languages:** English, German, Spanish, French, Italian, Dutch, Portuguese, Russian, Chinese, Swahili, Bengali, Telugu

**Data Format:**
- **Format:** JSON files
- **Fields:** `id`, `syllogism`, `validity` (bool), `plausibility` (bool, training only)
- **Training:** English only (low-resource setting)
- **Test:** Multilingual evaluation

**Example:**
```json
{
  "id": "0",
  "syllogism": "Not all canines are aquatic creatures known as fish. It is certain that no fish belong to the class of mammals. Therefore, every canine falls under the category of mammals.",
  "validity": false,
  "plausibility": true
}
```

**Sample Data Generation:**
```bash
python3 src/generate_data.py --subtask 1  # Generate sample data
```

---

## ğŸ“‘ Quick Navigation

**ğŸ†• New here?** â†’ [Quick Start](#-quick-start-5-minutes) (5 minutes)  
**ğŸ‘¥ Working in a team?** â†’ [Team Guide](#-contributing-for-teams) (avoid merge conflicts!)  
**ğŸ¯ Ready to contribute?** â†’ [Pick Your Level](#-how-to-contribute) (â­, â­â­, or â­â­â­)  
**ğŸ“š Need to learn?** â†’ [Learning Resources](#-learning-resources) (tutorials by level)  
**â“ Need help?** â†’ [Available Commands](#-available-commands) (CLI reference)

---

## ğŸ¯ The Challenge

**Main Problem:** Language models exhibit **content effect** - they confuse formal validity with plausibility.

**Example:**
```
Syllogism: "All dogs are mammals. All mammals breathe. Therefore, all dogs breathe."
- Validity: TRUE (logically valid)
- Plausibility: TRUE (matches world knowledge)
- Model prediction: âœ… Correct (easy case)

Syllogism: "All cats are reptiles. All reptiles are cold-blooded. Therefore, all cats are cold-blooded."
- Validity: TRUE (logically valid structure)
- Plausibility: FALSE (contradicts world knowledge)
- Model prediction: âŒ Often wrong! (content effect bias)
```

**Our Goal:** Build models that predict validity **independent** of plausibility.

### Subtasks

**Subtask 1: English Syllogistic Reasoning**
- Predict validity (True/False)
- Minimize content effect bias
- Metric: Accuracy / Content Effect ratio

**Subtask 2: English + Irrelevant Premises**
- Predict validity
- Identify relevant premises
- Metric: (Accuracy + F1) / Content Effect ratio

**Subtask 3: Multilingual Reasoning**
- Same as Subtask 1, but 12 languages
- Zero-shot transfer from English training

**Subtask 4: Multilingual + Irrelevant Premises**
- Same as Subtask 2, but 12 languages
- Most challenging task

---

## ğŸš€ Quick Start (5 minutes)

### 1. Setup (Reproducible Environment)

**Recommended: Use our setup script**
```bash
# Clone the repository
git clone <your-repo-url>
cd semeval_task11_complete

# Run automated setup
./setup_env.sh

# Activate environment
source venv/bin/activate

# Verify everything is correct
python3 verify_environment.py
```

### 2. Train Your First Model

```bash
# Train baseline model on Subtask 1
python3 src/pipeline.py train --subtask 1

# Expected output: ~60-70% accuracy with content effect

# Check dataset info
python3 src/pipeline.py info --subtask 1
```

### 3. Explore Data

```bash
# Open Jupyter notebook
jupyter notebook notebooks/01_data_exploration.ipynb
```

### 4. Run Tests

```bash
./run_all_tests.sh
# All tests should pass âœ…
```

---

## ğŸ“ Project Structure

```
semeval_task11_complete/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ RESEARCH.md               # Research direction & hypotheses
â”œâ”€â”€ CONTRIBUTING.md           # Contribution workflow
â”œâ”€â”€ requirements.txt          # Pinned dependencies
â”œâ”€â”€ setup_env.sh              # Environment setup script
â”œâ”€â”€ verify_environment.py     # Environment verification
â”‚
â”œâ”€â”€ src/                      # Core code
â”‚   â”œâ”€â”€ pipeline.py          # Main CLI
â”‚   â”œâ”€â”€ features/            # â­â­ Add YOUR feature file here (no conflicts!)
â”‚   â”‚   â”œâ”€â”€ basic.py         # Basic linguistic features
â”‚   â”‚   â”œâ”€â”€ logical.py       # Logical structure features
â”‚   â”‚   â”œâ”€â”€ _template.py     # Copy this to start
â”‚   â”‚   â””â”€â”€ yourname.py      # â† Create your own!
â”‚   â”œâ”€â”€ models/              # â­â­â­ Add YOUR model file here (no conflicts!)
â”‚   â”‚   â”œâ”€â”€ baseline.py      # Baseline models
â”‚   â”‚   â”œâ”€â”€ _template.py     # Copy this to start
â”‚   â”‚   â””â”€â”€ yourname.py      # â† Create your own!
â”‚   â”œâ”€â”€ data_loader.py       # Load syllogism data
â”‚   â”œâ”€â”€ evaluate.py          # Evaluation metrics
â”‚   â””â”€â”€ generate_data.py     # Sample data generation
â”‚
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb    # â­ Start here
â”‚   â””â”€â”€ 02_baseline_training.ipynb   # â­â­ Train models
â”‚
â”œâ”€â”€ experiments/              # â­â­â­ Large research projects
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ data/                     # Datasets
â””â”€â”€ models/                   # Saved models
```

**Your contributions fit here:**
- **â­ Beginner** â†’ `notebooks/` (explore data, visualize)
- **â­â­ Intermediate** â†’ `src/features/yourname.py` (add features - **no merge conflicts!**)
- **â­â­â­ Advanced** â†’ `src/models/yourname.py`, `experiments/` (transformers, novel methods)

---

## ğŸ‘¥ Contributing (For Teams)

**Working in a team of 10 students?** Here's how to organize:

### Role Distribution
- **Data/Features (3 students)** â†’ `notebooks/`, `src/features/alice.py`, `src/features/bob.py`
- **Modeling (3 students)** â†’ `src/models/dave.py`, `experiments/transformer_models/`
- **Multilingual (2 students)** â†’ Cross-lingual transfer, translation augmentation
- **Research (2 students)** â†’ Documentation, paper writing

**Key:** Each student gets their own file/folder â†’ **Zero merge conflicts!**

**How to add features/models:**
```bash
# Features: Copy template
cp src/features/_template.py src/features/yourname.py
# Edit, then register in src/features/__init__.py (2 lines)

# Models: Copy template
cp src/models/_template.py src/models/yourname.py
# Edit, then register in src/models/__init__.py (2 lines)
```

---

## ğŸ‘¤ How to Contribute

**Everyone can contribute!** We have tasks for all skill levels.

### â­ Beginner Tasks (No ML experience needed)

#### 1. Data Exploration (2-5 hours)
- **Task:** Analyze syllogism patterns in training data
- **What to do:**
  - Count valid vs invalid arguments
  - Analyze plausible vs implausible distribution
  - Visualize content effect in training data
- **Skills learned:** Data analysis, visualization
- **File:** `notebooks/yourname_exploration.ipynb`

#### 2. Documentation (1-2 hours)
- **Task:** Improve README or add examples
- **What to do:**
  - Add more syllogism examples
  - Document evaluation metrics clearly
  - Create beginner-friendly guides
- **Skills learned:** Technical writing

### â­â­ Intermediate Tasks (Some ML/NLP experience)

#### 1. Linguistic Features (5-10 hours)
- **Task:** Extract linguistic features from syllogisms
- **What to do:**
  - POS tagging, dependency parsing
  - Negation detection
  - Quantifier extraction ("all", "some", "no")
- **Skills learned:** NLP, feature engineering
- **Expected impact:** +5-10% accuracy

#### 2. Logical Structure Features (5-10 hours)
- **Task:** Extract formal logic patterns
- **What to do:**
  - Identify premise-conclusion structure
  - Extract logical connectives
  - Detect syllogistic figures (AAA-1, EAE-2, etc.)
- **Skills learned:** Logic, pattern matching
- **Expected impact:** +10-15% accuracy

#### 3. Multilingual Evaluation (5-8 hours)
- **Task:** Test models across languages
- **What to do:**
  - Evaluate on different language subsets
  - Analyze cross-lingual performance
  - Identify language-specific biases
- **Skills learned:** Multilingual NLP

### â­â­â­ Advanced Tasks (Research/Deep Learning)

#### 1. Transformer Fine-tuning (10-20 hours)
- **Task:** Fine-tune multilingual transformers
- **What to do:**
  - Fine-tune mBERT, XLM-R on syllogisms
  - Experiment with prompt engineering
  - Implement chain-of-thought reasoning
- **Skills learned:** Transformers, prompt engineering
- **Expected impact:** +20-30% accuracy

#### 2. Debiasing Methods (15-25 hours)
- **Task:** Reduce content effect bias
- **What to do:**
  - Implement contrastive learning
  - Try adversarial training
  - Explore causal intervention methods
- **Skills learned:** Advanced ML, bias mitigation
- **Expected impact:** -20-40% content effect

#### 3. Premise Selection (10-15 hours)
- **Task:** Build models for Subtask 2/4
- **What to do:**
  - Implement attention-based selection
  - Try graph neural networks
  - Multi-task learning (validity + selection)
- **Skills learned:** Structured prediction

---

## ğŸ”¬ Research & Advanced Topics

**For detailed research direction, hypotheses, and experimental plans:**
ğŸ‘‰ **See [RESEARCH.md](RESEARCH.md)**

**Key research questions:**
- How to disentangle content from formal reasoning?
- Can we achieve zero-shot multilingual transfer?
- What features best predict validity independent of plausibility?

---

## ğŸ¤ How to Contribute

**For detailed contribution workflow and git guidelines:**
ğŸ‘‰ **See [CONTRIBUTING.md](CONTRIBUTING.md)**

**Quick workflow:**
1. Pick a task from GitHub Issues
2. Create branch: `git checkout -b feature/your-task`
3. Make changes and test
4. Push and open Pull Request
5. Git tracks your contributions automatically!

---

## ğŸ”§ Available Commands

### Training

```bash
# Train on Subtask 1 (English)
python3 src/pipeline.py train --subtask 1

# Train on Subtask 2 (English + irrelevant premises)
python3 src/pipeline.py train --subtask 2

# Train specific model
python3 src/pipeline.py train --subtask 1 --model-type transformer
```

### Evaluation

```bash
# Evaluate on validation set
python3 src/pipeline.py evaluate --subtask 1 --model-path models/subtask1_model.pkl

# Calculate content effect
python3 src/evaluate.py --predictions results/predictions.json --gold data/val.json
```

### Data Info

```bash
# Show dataset statistics
python3 src/pipeline.py info --subtask 1
```

### Testing

```bash
# Run all tests
./run_all_tests.sh

# Run specific test
python3 tests/test_features.py
```

---

## ğŸ† Competition Details

### Evaluation Metrics

**Subtask 1 & 3:**
- **Accuracy:** Correct validity predictions
- **Content Effect:** Bias towards plausibility
- **Ranking:** Accuracy / Content Effect ratio (higher is better)

**Subtask 2 & 4:**
- **Accuracy:** Correct validity predictions
- **F1 Score:** Premise selection quality
- **Content Effect:** Bias towards plausibility
- **Ranking:** (Accuracy + F1) / 2 / Content Effect ratio

### Timeline

| Date | Event |
|------|-------|
| **Now - Dec 2025** | Training phase |
| **Dec 2025** | Test data released |
| **Jan 2026** | Evaluation period |
| **Feb 2026** | Paper submission |
| **Summer 2026** | SemEval workshop |

---

## ğŸ‘¥ Team & Authorship

**Authorship will be determined during paper writing based on:**
- Quality and impact of contributions
- Sustained engagement throughout the project
- Collaboration with team members
- Git commit history as supporting evidence

**Typical tiers:**
- **ğŸ¥‡ Lead/Co-Authors:** Major features, sustained engagement, significant impact
- **ğŸ¥ˆ Contributing Authors:** Meaningful contributions, consistent involvement
- **ğŸ¥‰ Acknowledged:** Helpful contributions, bug fixes, documentation

---

## ğŸ“š Learning Resources

### Logical Reasoning
- **Syllogistic Logic:** [Stanford Encyclopedia](https://plato.stanford.edu/entries/logic-classical/)
- **Formal Logic Basics:** [Khan Academy](https://www.khanacademy.org/math/logic)

### NLP & Transformers
- **Transformers:** [HuggingFace Course](https://huggingface.co/course)
- **Multilingual NLP:** [mBERT Paper](https://arxiv.org/abs/1810.04805)
- **XLM-RoBERTa:** [Paper](https://arxiv.org/abs/1911.02116)

### Bias & Debiasing
- **Content Effect:** [Task 11 References](https://sites.google.com/view/semeval-2026-task-11)
- **Debiasing Methods:** [Survey Paper](https://arxiv.org/abs/2103.00453)

### Tools
- **spaCy:** [NLP Library](https://spacy.io/)
- **Transformers:** [HuggingFace](https://huggingface.co/transformers/)
- **PyTorch:** [Deep Learning](https://pytorch.org/)

---

## ğŸ’¬ Getting Help

- **Questions?** Open an issue with `question` label
- **Found a bug?** Open an issue with `bug` label
- **Have an idea?** Open an issue with `enhancement` label
- **Task Slack:** [Join here](https://sites.google.com/view/semeval-2026-task-11)

---

## ğŸ“ Learning Outcomes

By contributing to this project, you will:
- âœ… Understand formal logic and reasoning
- âœ… Learn about content bias in language models
- âœ… Work with multilingual NLP
- âœ… Gain experience with transformers
- âœ… Contribute to a published research paper
- âœ… Build your research portfolio

---

## ğŸš€ Ready to Start?

1. **Read this README** (you're doing it! âœ…)
2. **Pick your difficulty level** (â­, â­â­, or â­â­â­)
3. **Follow Quick Start** (5 minutes)
4. **Choose a task** (check Issues)
5. **Make your contribution** (follow workflow)
6. **Commit and push** (Git tracks everything automatically!)

**Questions?** Run `python3 src/pipeline.py --help` or open an issue.

---

## ğŸ”’ Reproducibility

### Environment Reproducibility

**We ensure "works on my machine" NEVER happens:**

1. **Pinned Dependencies** - All package versions exact in `requirements.txt`
2. **Python Version Control** - `.python-version` specifies Python 3.9.18
3. **Virtual Environment** - Isolated from system Python
4. **Environment Verification** - `verify_environment.py` checks everything

### Experimental Reproducibility

All experiments are **fully reproducible** with seed `42`:
- âœ… Data generation produces identical datasets
- âœ… Model training is deterministic
- âœ… Results are consistent across runs

**To reproduce results:**
```bash
# 1. Setup environment (once)
./setup_env.sh
source venv/bin/activate

# 2. Verify environment
python3 verify_environment.py

# 3. Run experiments (same results every time)
python3 src/generate_data.py --subtask 1
python3 src/pipeline.py train --subtask 1
```

**Guaranteed:** Same environment + same seed = same results!

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details

---

**Let's disentangle content from reasoning together!** ğŸ¯
